{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> (AI4LR) - Notebook 2\n\nTotal Points: 50\n\nThis notebook is designed to get you fimiliar with the LI, L2 Loss Functions and Logistic regression. ","metadata":{}},{"cell_type":"markdown","source":"## Linear and Logistic regression","metadata":{"id":"e45e2682","editable":false}},{"cell_type":"code","source":"# performing Imports\n\nimport numpy as np # numpy is used mostly for the algebraic operations\nimport os # to perform operations related to system\nimport math # used to perform mathmatical operations","metadata":{"id":"2e50065b","editable":false,"execution":{"iopub.status.busy":"2022-03-15T07:19:24.09661Z","iopub.execute_input":"2022-03-15T07:19:24.097023Z","iopub.status.idle":"2022-03-15T07:19:24.100484Z","shell.execute_reply.started":"2022-03-15T07:19:24.096994Z","shell.execute_reply":"2022-03-15T07:19:24.099973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Q1 (12 points)\n\n>1.1    Define a sigmoid function \"sigmoid\" that takes a number as the input, apply sigmoid function on the input and returns the value after applying sigmoid function. (3)\n\n>1.2  Initialize a numpy array \"ran_arr\" with 50 random entries between 0-10. (2)\n\n>1.3  Initialize one empty list \"log_list\".(2)\n\n>1.4  Create a \"for\" loop to pass all the values in the \"ran_arr\", apply \"sigmoid\" function and store all the values in \"log_list\" (3) \n\n>1.5   Print \"log_list\"","metadata":{"id":"91026b4f","editable":false}},{"cell_type":"code","source":"## Complete the Code below\n\n# define function\ndef sigmoid(num):\n    # input: array of float numbers\n    # return: \n    return 1 / (1 + np.exp(-num))\n\n# initialize array\nran_arr = np.random.uniform(low=0, high=10, size=(1,50)) \n\n# initialize list\nlog_list = []\n\n# for loop to apply the logistic function on all values of ran_arr \nfor number in ran_arr:\n    log_list=sigmoid(number) \n\n# print()\nprint(log_list)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Q2 Implement the L1 loss functions (8 points)\n\n**Hint**: Implement the numpy vectorized version of the L1 loss. You may find the function abs(x) (absolute value of x) useful.\n\n- The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions ($ \\hat{y} $) are from the true values ($y$). \n- In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.\n\n### L1 loss is defined as:\n\n\n$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}$$\n\n","metadata":{"id":"62d161c9","editable":false}},{"cell_type":"code","source":"#  Implement l1_loss function here\n\ndef L1_loss(y_cap, y):\n    \"\"\"\n    y_cap: vector of size m (predicted labels)\n    y: vector of size m (true labels)\n    \n    Returns:\n    loss: the value of the L1 loss function defined above\n    \"\"\"\n    \n    ### code here\n    loss = np.sum(np.abs(y_cap - y),axis = 0)\n    return loss\n\ny_cap = np.array([.8, 0.2, 0.3, .9, .9])\ny = np.array([1, 0, 0, 1, 1])\nprint(\"L1_loss = \" + str(L1_loss(y_cap,y)))","metadata":{"id":"0dbef630","editable":false,"execution":{"iopub.status.busy":"2022-03-15T07:19:44.124421Z","iopub.execute_input":"2022-03-15T07:19:44.124752Z","iopub.status.idle":"2022-03-15T07:19:44.131695Z","shell.execute_reply.started":"2022-03-15T07:19:44.124692Z","shell.execute_reply":"2022-03-15T07:19:44.131042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Q3. Implement the L2 loss functions (6 points)\n\nImplement the numpy vectorized version of the L2 loss. \nHint: There are several way of implementing the L2 loss but you may find the function np.dot() useful. \n- As a reminder, if $x = [x_1, x_2, ..., x_n]$, then `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$. \n\n### L2 loss is defined as \n\n$$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}$$","metadata":{"id":"1hGn0RXacy8B","editable":false}},{"cell_type":"code","source":"#  implement l2_loss here\n\ndef L2_loss(y_cap, y):\n    \"\"\"\n    \n    y_cap: vector of size m (predicted labels)\n    y: vector of size m (true labels)\n    \n    Returns:\n    loss: the value of the L2_loss function defined above\n    \"\"\"\n    \n    ### code here\n    loss = np.dot(np.abs(y_cap - y),np.abs(y_cap - y))\n    return loss\n    \n\ny_cap = np.array([.9, 0.2, 0.1, .4, .9])\ny = np.array([1, 0, 0, 1, 1])\nprint(\"L2_loss = \" + str(L2_loss(y_cap,y)))","metadata":{"id":"cf18591c","editable":false,"execution":{"iopub.status.busy":"2022-03-15T07:20:04.698738Z","iopub.execute_input":"2022-03-15T07:20:04.699555Z","iopub.status.idle":"2022-03-15T07:20:04.706521Z","shell.execute_reply.started":"2022-03-15T07:20:04.699502Z","shell.execute_reply":"2022-03-15T07:20:04.705623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Q4 (2 Points) \nLogistic Regression is a Machine Learning algorithm that is used to predict the probability of a ___ ? \n\n>(A) categorical independent variable.\n\n>(B) categorical dependent variable.\n\n>(C) numerical dependent variable.\n\n>(D) numerical independent variable.","metadata":{}},{"cell_type":"code","source":"# Print the correct option\nprint(\"B\")","metadata":{"id":"MinbQAj2C9wk","editable":false,"execution":{"iopub.status.busy":"2022-03-15T07:20:09.493827Z","iopub.execute_input":"2022-03-15T07:20:09.494365Z","iopub.status.idle":"2022-03-15T07:20:09.498418Z","shell.execute_reply.started":"2022-03-15T07:20:09.49433Z","shell.execute_reply":"2022-03-15T07:20:09.497693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q5 (2 points) \nIn a logistic regression model, the decision boundary can be ___ ? \n\n>(A) categorical independent variable.\n\n>(B) categorical dependent variable.\n\n>(C) numerical dependent variable.\n\n>(D) numerical independent varia","metadata":{}},{"cell_type":"code","source":"# Print your answer\nprint(\"C\")","metadata":{"id":"gSZddzFuFWQm","editable":false,"execution":{"iopub.status.busy":"2022-03-15T07:20:12.628442Z","iopub.execute_input":"2022-03-15T07:20:12.628752Z","iopub.status.idle":"2022-03-15T07:20:12.633024Z","shell.execute_reply.started":"2022-03-15T07:20:12.628719Z","shell.execute_reply":"2022-03-15T07:20:12.632124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q6 (15 points) \n\nCreate a dataset by taking 500 random values between 0 to 2. It would be the input array X. \n\nGenerate y using the below equation\n\ny = (X-1)(X-2)(X-3)\n\n#### Use below helper functions to train your logistic regression model for 5000 iterations and predict values for y and print the 50 most accurate predictions.\n\nNote: Accurate prediction will be judged on the basis of the difference between predicted and original y values. \n\n\n```\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef compute_cost(X, y, theta):\n    m = len(y)\n    y_cap = sigmoid(X @ theta)\n    \n    cost = (1/m)*(((-y).T @ np.log(y_cap ))-((1-y).T @ np.log(1-y_cap)))\n    return cost\n\ndef gradient_descent(X, y, params, learning_rate, iterations):\n    m = len(y)\n    cost_history = np.zeros((iterations,1))\n    for i in range(iterations):\n        params = params - (learning_rate/m) * (X.T @ (sigmoid(X @ params) - y)) \n        cost_history[i] = compute_cost(X, y, params)\n    return (cost_history, params)\n\ndef predict(X, params):\n    return np.round(sigmoid(X @ params))\n```\n\n","metadata":{"id":"Lr7kcxbDnChN","editable":false}},{"cell_type":"code","source":"#import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n#generate X\nX = np.random.uniform(0,2,500)\n\n#generate y\ngen = lambda t: (t-1)*(t-2)*(t-3)\ngeny = np.array([gen(i) for i in X])\n\ngen_sort = np.copy(geny)\nprint(gen_sort.shape)\n\nmin_val = gen_sort.min()\nmax_val = gen_sort.max()\ngen_sort.sort()\n\nimport statistics\nmedian = statistics.median(gen_sort)\n\n#define criteria to convert y to binary\ny_list = [0 if i <=median else 1 for i in geny]\ny = np.array(y_list)\n\n#define all required functions \ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef compute_cost(X, y, W,b):\n    m = len(y)\n    y_cap = sigmoid(X*W+b) \n    cost = (1/m)*(((-y).T@np.log(y_cap ))-((1-y).T @ np.log(1-y_cap)))\n    return cost\n\ndef gradient_descent(X, y, W,b, learning_rate, iterations):\n    m = len(y)\n    cost_history = np.zeros((iterations,1))\n    for i in range(iterations):\n        W = W - (learning_rate/m) * (X.T @ (sigmoid(X*W+b) - y))\n        b = b - (learning_rate/m) * (np.sum(sigmoid(X*W+b) - y))\n        cost_history[i] = compute_cost(X, y, W,b) \n    return (cost_history, W,b) \n\ndef predict(X, W,b):\n    return np.round(sigmoid(X*W+b))\n\n#initialize all required parameters \nW=0\nb=0\niterations = 5000\nlearning_rate = 0.03\n\n#calculate cost\ninitial_cost = compute_cost(X, y, W,b)\nprint(\"Initial Cost is: {} \\n\".format(initial_cost))\n(cost_history, W_optimal, b_optimal) = gradient_descent(X, y, W,b, learning_rate, iterations)\nW_optimal, b_optimal\n\n#plot cost function\nplt.figure()\nplt.plot(range(len(cost_history)), cost_history, 'r')\nplt.title(\"Convergence Graph of Cost Function\")\nplt.xlabel(\"Number of Iterations\")\nplt.ylabel(\"Cost\")\nplt.show()\n\n#predict\ny_pred = predict(X, W_optimal, b_optimal)\nscore = float(sum(y_pred == np.round(y)))/ float(len(y))\nprint(score)\n\ny_pred[:50]","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:29:07.066929Z","iopub.execute_input":"2022-03-15T05:29:07.067426Z","iopub.status.idle":"2022-03-15T05:29:07.820984Z","shell.execute_reply.started":"2022-03-15T05:29:07.067385Z","shell.execute_reply":"2022-03-15T05:29:07.820217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Q7 (3 points)\nIn which domain(eg. Agriculture) other than space would you like to apply the concepts you have learned so far and how ?\n(Atleast 100 Words)","metadata":{"id":"62d7c411","editable":false}},{"cell_type":"code","source":"# Put your thoughts\n\nyour_answer = \"\" \n\nprint(your_answer)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:48:12.043117Z","iopub.execute_input":"2022-03-15T06:48:12.043597Z","iopub.status.idle":"2022-03-15T06:48:12.050149Z","shell.execute_reply.started":"2022-03-15T06:48:12.04356Z","shell.execute_reply":"2022-03-15T06:48:12.04892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q8 (2 points)\nIf you are given a chance to be a part of any space mission, which one it could be ? and why do you want to be a part of it?(Atleast 100 Words)","metadata":{"id":"cfe070fa","editable":false}},{"cell_type":"code","source":"# Put your thoughts\n\nyour_answer = \"remove this string and write your answer\" \n\nprint(your_answer)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T07:19:09.324651Z","iopub.execute_input":"2022-03-15T07:19:09.325054Z","iopub.status.idle":"2022-03-15T07:19:09.330336Z","shell.execute_reply.started":"2022-03-15T07:19:09.325022Z","shell.execute_reply":"2022-03-15T07:19:09.329581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thank you for completing all the questions","metadata":{}}]}