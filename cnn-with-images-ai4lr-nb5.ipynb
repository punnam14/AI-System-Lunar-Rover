{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> (AI4LR) - Notebook 5\nTotal Points: 50\n\nThis notebook is designed to get you fimiliar with neural networks and working with images.","metadata":{"id":"elcpXOswLRgc"}},{"cell_type":"markdown","source":"## Open this notebook in kaggle for easy access to the dataset and related libraries. ","metadata":{}},{"cell_type":"code","source":"# importing libraries\nimport tensorflow as tf\nimport keras\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport cv2\nfrom PIL import Image\nfrom numpy import asarray\nfrom matplotlib import image","metadata":{"id":"f5ayJkkRwkjb","execution":{"iopub.status.busy":"2022-04-03T13:02:13.675855Z","iopub.execute_input":"2022-04-03T13:02:13.676211Z","iopub.status.idle":"2022-04-03T13:02:16.045126Z","shell.execute_reply.started":"2022-04-03T13:02:13.676124Z","shell.execute_reply":"2022-04-03T13:02:16.044405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data \n\nTo get the dataset for this assignment, got to Add data option and search for https://www.kaggle.com/datasets/datatangai/people-with-occlusion-and-multipose-face-data in Search URL field.\n\nImport this dataset and there is only one image in this dataset. We are going to use this image only. ","metadata":{"id":"d72b-W8fFaKI"}},{"cell_type":"code","source":"#img_path = \"../input/people-with-occlusion-and-multipose-face-data/3.png\"\nimg_path = \"../input/face-data-1/3.png\"\n\nimg = Image.open(img_path)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T13:03:32.643111Z","iopub.execute_input":"2022-04-03T13:03:32.643594Z","iopub.status.idle":"2022-04-03T13:03:32.659223Z","shell.execute_reply.started":"2022-04-03T13:03:32.643561Z","shell.execute_reply":"2022-04-03T13:03:32.658240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q1 (1 Point) \n\nConvert the images \"img\" into an array and store that array into 'img_arr' variable.\n\nPlot the img_arr using plt\n","metadata":{"id":"QAhrBKqmjuHB"}},{"cell_type":"code","source":"# Complete the following code\n\nimg_arr = asarray(img)\nprint(type(img_arr))\nprint(img_arr.shape)\n\n# plot img_arr\n\nplt.imshow(img_arr, interpolation='nearest')\nplt.show()","metadata":{"id":"ShSSOgg350kW","execution":{"iopub.status.busy":"2022-04-03T13:03:36.796463Z","iopub.execute_input":"2022-04-03T13:03:36.796767Z","iopub.status.idle":"2022-04-03T13:03:37.209485Z","shell.execute_reply.started":"2022-04-03T13:03:36.796736Z","shell.execute_reply":"2022-04-03T13:03:37.208417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q2 (2 Points)\nRun the below code.\n\nDo you see anything abnormal with the shape of the above image? If yes, state the reason. ","metadata":{"id":"KqNymgaxpk1i"}},{"cell_type":"code","source":"print(\"Image shape:\", img_arr.shape)\n\n## print your reason here\n\nreason = \"four channel image\"\n\nprint(reason)","metadata":{"id":"23zzRR39pj-X","execution":{"iopub.status.busy":"2022-04-03T13:03:44.505685Z","iopub.execute_input":"2022-04-03T13:03:44.506017Z","iopub.status.idle":"2022-04-03T13:03:44.511815Z","shell.execute_reply.started":"2022-04-03T13:03:44.505976Z","shell.execute_reply":"2022-04-03T13:03:44.510830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q3 (6 Points)\nThe above image \"img_arr\" is a combination of ten different images, separate all ten images using numpy and show all of them using plt.subplot.\n\n(the size and shape of individual images should remain the same)","metadata":{"id":"0wj839JEq2vN"}},{"cell_type":"code","source":"# first we are converting our four channels image into three channels image (easy to use and no extra complexities)\n'''Do not make any change in this below line of code.'''\nimg_arr = img_arr[:, :, :3]\n\n\n# Write your code here--------------------------->\nimage1 = img_arr[:500, :290, :]\nimage2 = img_arr[:500, 290:580, :]\nimage3 = img_arr[:500, 580:870, :]\nimage4 = img_arr[:500, 870:1160, :]\nimage5 = img_arr[:500, 1160:1450, :]\nimage6 = img_arr[500:1000, :290, :]\nimage7 = img_arr[500:1000, 290:580, :]\nimage8 = img_arr[500:1000, 580:870, :]\nimage9 = img_arr[500:1000, 870:1160, :]\nimage10 = img_arr[500:1000, 1160:1450, :]\nplt.imshow(image1, interpolation='nearest')\nplt.show()","metadata":{"id":"G979ANNupyoY","execution":{"iopub.status.busy":"2022-04-03T13:04:26.588420Z","iopub.execute_input":"2022-04-03T13:04:26.588737Z","iopub.status.idle":"2022-04-03T13:04:26.732545Z","shell.execute_reply.started":"2022-04-03T13:04:26.588702Z","shell.execute_reply":"2022-04-03T13:04:26.731641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n\ninfile = '../input/face-data-1/3.png'\nchopsize = 300\n\nimg = Image.open(infile)\nwidth, height = img.size\n\n# Save Chops of original image\nfor x0 in range(0, width, chopsize):\n    for y0 in range(0, height, chopsize):\n        box = (x0, y0, \n               x0+chopsize if x0+chopsize <  width else  width - 1,\n               y0+chopsize if y0+chopsize < height else height - 1)\n        print('%s %s' % (infile, box))\n        img.crop(box).save('../input/face-data-1' % (infile.replace('../input/face-data-1/3.png',''), x0, y0))","metadata":{"execution":{"iopub.status.busy":"2022-04-03T13:04:32.783637Z","iopub.execute_input":"2022-04-03T13:04:32.783986Z","iopub.status.idle":"2022-04-03T13:04:33.144503Z","shell.execute_reply.started":"2022-04-03T13:04:32.783948Z","shell.execute_reply":"2022-04-03T13:04:33.143267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q4 (4 Points)\nimgGray = ** 0.2989 * R + 0.5870 * G + 0.1140 * B **. \n\nthis is the formula to convert any colored images to greyscale. \n\n\nConvert \"first\" into greyscale using above formula and store it in \"first_bw\"","metadata":{"id":"wYxLSwSWrgey"}},{"cell_type":"code","source":"first = img_arr[:500, :290, :]\n\nplt.figure(figsize = (5,10))\nplt.imshow(first)\nplt.show()\n\n## Write your code here\nR, G, B = first[:,:,0], first[:,:,1], first[:,:,2]\nfirst_bw = 0.2989 * R + 0.5870 * G + 0.1140 * B\nplt.imshow(first_bw, cmap='gray')\nplt.show()","metadata":{"id":"482aUQZeraMd","execution":{"iopub.status.busy":"2022-04-02T17:02:43.618072Z","iopub.execute_input":"2022-04-02T17:02:43.618387Z","iopub.status.idle":"2022-04-02T17:02:44.022859Z","shell.execute_reply.started":"2022-04-02T17:02:43.618356Z","shell.execute_reply":"2022-04-02T17:02:44.021683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q5 (6 Points)\nFind out the effects of kernel1 and kernel2 on the \"first\" using cv2.filter2D function. \n\nPlot and State the difference between effect1 and effect2.\n","metadata":{"id":"kiStl1oc6O5O"}},{"cell_type":"code","source":"kernel1 = np.array([[-1,-1,-1],\n                    [0,0,0],\n                    [1,1,1]])\n\nkernel2 = kernel1.transpose()\n\n\neffect1 = cv2.filter2D(src = first, kernel = kernel1, ddepth = -1)\neffect2 = cv2.filter2D(src = first, kernel = kernel2, ddepth = -1)\n\n# plot effect1 and effect2 using plt.subplots\n\n\n\n\n# state the difference\n\ndifference = \"Kernel 1 This mask will find edges in the horizontal direction - because the zeros column is in the horizontal direction. This mask will prominent the horizontal edges in an image. It calculates the difference among the pixel intensities of a particular edge. As the center row consists of zeros it does not include the original values of edge in the image but rather it calculates the difference of above and below pixel intensities of the particular edge. Thus increasing the sudden change of intensities and making the edge more visible. Kernel 2 The mask will find the edges in vertical direction - because the zeros column is in the vertical direction. When we apply this mask on the image it makes the vertical edges prominent. It simply works like as first order derivate and calculates the difference of pixel intensities in a edge region. As the center column is of zero so it does not include the original values of an image but rather it calculates the difference of right and left pixel values around that edge. This increases the edge intensity and it becomes enhanced compared to the original image.Together applied to an image, it makes up the Prewitt operatorBoth the above masks follow the principle of derivate mask. Both masks have opposite sign in them and both masks sum equals to zero. \"\n\"\"\"\nKernel 1\nThis mask will find edges in the horizontal direction - because the zeros column is in the horizontal direction. \nThis mask will prominent the horizontal edges in an image. It calculates the difference among the pixel intensities of a particular edge. \nAs the center row consists of zeros it does not include the original values of edge in the image but rather it calculates the difference of \nabove and below pixel intensities of the particular edge. Thus increasing the sudden change of intensities and making the edge more visible. \n\nKernel 2 \nThe mask will find the edges in vertical direction - because the zeros column is in the vertical direction. \nWhen we apply this mask on the image it makes the vertical edges prominent. It simply works like as first order derivate and \ncalculates the difference of pixel intensities in a edge region. As the center column is of zero so it does not include the \noriginal values of an image but rather it calculates the difference of right and left pixel values around that edge. \nThis increases the edge intensity and it becomes enhanced compared to the original image.\n\n\nTogether applied to an image, it makes up the Prewitt operator\nBoth the above masks follow the principle of derivate mask. Both masks have opposite sign in them and both masks sum equals to zero. \n\"\"\"\n\nprint(difference)\n","metadata":{"id":"TbY77UkX67Xv","execution":{"iopub.status.busy":"2022-04-02T17:02:52.862626Z","iopub.execute_input":"2022-04-02T17:02:52.862962Z","iopub.status.idle":"2022-04-02T17:02:52.90311Z","shell.execute_reply.started":"2022-04-02T17:02:52.862929Z","shell.execute_reply":"2022-04-02T17:02:52.901778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(effect1)\nplt.show()\nplt.imshow(effect2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-02T17:02:59.343786Z","iopub.execute_input":"2022-04-02T17:02:59.344182Z","iopub.status.idle":"2022-04-02T17:02:59.600356Z","shell.execute_reply.started":"2022-04-02T17:02:59.344146Z","shell.execute_reply":"2022-04-02T17:02:59.599055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q6 (4 Points)\n\nState differences between -  \n\n> Feature extraction and finetuning in transfer learning.\n\n> Hidden layer in a neural network and Head layer in transfer learning ","metadata":{"id":"Qh-Tu3DrGf7Y"}},{"cell_type":"code","source":"# start your answer here\n\nanswer1 = \"Feature Extraction aims to reduce the number of features in a dataset by creating new features from the existing ones (and then discarding the original features). These new reduced set of features should then be able to summarize most of the information contained in the original set of features.\" \n\nanswer2 = \"Transfer learning consists of taking features learned on one problem and leveraging them on a similar problem. After we import layers from a previously trained model we add new trainable layers on top of the frozen layers. In fine-tuning we unfreeze the entire model or part of it and re-train it on the new data with a very low learning rate. This incrementally adapts the pretrained features to the new data\"\n\nprint(answer1)\nprint()\nprint(answer2)\n","metadata":{"id":"R1ja-IvR4d34","execution":{"iopub.status.busy":"2022-04-02T17:33:20.023686Z","iopub.execute_input":"2022-04-02T17:33:20.024139Z","iopub.status.idle":"2022-04-02T17:33:20.032189Z","shell.execute_reply.started":"2022-04-02T17:33:20.024099Z","shell.execute_reply":"2022-04-02T17:33:20.030901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q7 (2 points)\n\nWrite the formula for IOU score using numpy.","metadata":{"id":"s7S2hsT-7eu-"}},{"cell_type":"code","source":"# Print your answer\n\nformula = \"np.sum(intersection) / np.sum(union)\" \nprint(formula)","metadata":{"id":"_qcETSSR8866","execution":{"iopub.status.busy":"2022-04-02T20:51:02.841267Z","iopub.execute_input":"2022-04-02T20:51:02.841556Z","iopub.status.idle":"2022-04-02T20:51:02.846074Z","shell.execute_reply.started":"2022-04-02T20:51:02.841526Z","shell.execute_reply":"2022-04-02T20:51:02.845155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q8 (10 points)\n\nFor below model, find out the total number of **trainable parameters** and print **all the model layers**.\n","metadata":{"id":"0mL9NMduQFg5"}},{"cell_type":"code","source":"from keras.engine.sequential import Sequential\nfrom keras.layers import Input, Conv2D, Dense, MaxPooling2D, Dropout\n\nmodel = Sequential([\n                    Input(shape = (500, 290, 3)),\n                    Conv2D(16, (3,3), activation = 'relu'),\n                    MaxPooling2D(2,2),\n                    Conv2D(32, (3,3), activation = 'relu'),\n                    MaxPooling2D(2,2),\n                    Conv2D(64, (3,3), activation = 'relu'),\n                    MaxPooling2D(2,2),\n                    Dense(256, activation = \"relu\"),\n                    Dropout(0.5),\n                    Dense(5, activation = \"sigmoid\")\n])\n\nfrom keras.utils.layer_utils import count_params  \n\ntrain_params = count_params(model.trainable_weights)\nprint(\"Trainable parameters:\", train_params)","metadata":{"id":"HUVQCUDXP_Rm","execution":{"iopub.status.busy":"2022-04-02T19:27:10.564977Z","iopub.execute_input":"2022-04-02T19:27:10.566028Z","iopub.status.idle":"2022-04-02T19:27:10.665852Z","shell.execute_reply.started":"2022-04-02T19:27:10.565972Z","shell.execute_reply":"2022-04-02T19:27:10.664775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T19:27:13.562769Z","iopub.execute_input":"2022-04-02T19:27:13.563127Z","iopub.status.idle":"2022-04-02T19:27:13.69332Z","shell.execute_reply.started":"2022-04-02T19:27:13.563085Z","shell.execute_reply":"2022-04-02T19:27:13.692143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q9 (15 points)\nFor a Convolutional layer in neural network, below values are given\n\n> Input shape = (64, 64, 3)\n\n> Convolutional filter size = (3,3)\n\n> Number of Conv Filters = 16\n\n> padding = \"same\"\n\n> stride = 1\n\n\nFind out the **output shape** after this layer, **number of total trainable paramaters** and **computational cost**.\n\nHint: Total trainable parameters is the sum of total number of weights and biases in the layer.\n\n\nHint: Computational cost is the total number of matrics multiplication ","metadata":{"id":"wJMt9xKSTQsf"}},{"cell_type":"code","source":"# write your answer here\n# when padding=\"same\" and stride=1, the output has the same size as the input.\nout_shape = (64,64,3)\n\n#(num_filters * filter_size * filter_size * num_channels) + num_filters\ntrain_params = 448 \n\n#64*64*16*3*3*3\ncomp_cost = 1,769,472 #1.7 million\n\nprint(\"Output shape:\", out_shape)\nprint(\"Trainable parameters:\", train_params)\nprint(\"Computational Cost:\", comp_cost)","metadata":{"id":"lt1qr2GZTP63","execution":{"iopub.status.busy":"2022-04-02T20:50:06.993006Z","iopub.execute_input":"2022-04-02T20:50:06.993961Z","iopub.status.idle":"2022-04-02T20:50:07.002556Z","shell.execute_reply.started":"2022-04-02T20:50:06.993871Z","shell.execute_reply":"2022-04-02T20:50:07.001498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Thank you for completing all the questions","metadata":{"id":"wIiqmkDHRLwC"}}]}