{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> (AI4LR) - Notebook 3\n    \nTotal Points: 50\n\nThis notebook is designed to get you fimiliar with neural networks.","metadata":{"id":"IZUj9ctCef14"}},{"cell_type":"markdown","source":"**Ques** 1(3 marks) Which one of the following is not an activation function\n* a - Sigmoid function\n* b - Hyperbolic tangent function\n* c - Rectified linear unit (RELU) function\n* d - Leaky RELU function\n* e - dynamic function\n* f - Maxout function\n* g - gaussian function\n* h - sobel function\n* i - Exponential Linear unit (ELU) function","metadata":{"id":"lAgJFEblw4vX"}},{"cell_type":"code","source":"print(\"E G H \")# print the correct options here","metadata":{"id":"pauXcMhfx3Uj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ques** 2 (5 marks) Write a function to initialize a neural network having 2 inputs, 1 hidden layer and 2 outputs and print the layers of network","metadata":{"id":"Q_Z00qCgyAHC"}},{"cell_type":"code","source":"# Initialize a network\ndef initialize_network(n_inputs, n_hidden, n_outputs):\n    network = list()\n    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n    network.append(hidden_layer)\n    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n    network.append(output_layer)\n    return network\n","metadata":{"id":"ulYC3aB2yVoZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ques** 3 (7 marks) Write a function for neuron activation. Also call the function and print values\n\nactivation = sum(weight_i * input_i) + bias","metadata":{"id":"uBpgre59y0FP"}},{"cell_type":"code","source":"# Calculate neuron activation for an input\ndef activate(weights, inputs):\n    activation = weights[-1]\n    for i in range(len(weights)-1):\n        activation += weights[i] * inputs[i]\n    return activation","metadata":{"id":"O1bAGMtHzCUr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ques** 4 (15 marks) Write a function to calculate backpropogation error\nTest the error ( delta for given inputs)\n\n\nerror = (weight_k * error_j) * transfer_derivative(output)","metadata":{"id":"NhYVhAJRzcU-"}},{"cell_type":"code","source":"# Calculate the derivative of an neuron output\ndef transfer_derivative(output):\n    return output * (1.0 - output)\n # Backpropagate error and store in neurons\ndef backward_propagate_error(network, expected):\n    for i in reversed(range(len(network))):\n        layer = network[i]\n        errors = list()\n        if i != len(network)-1:\n            for j in range(len(layer)):\n                error = 0.0\n                for neuron in network[i + 1]:\n                    error += (neuron['weights'][j] * neuron['delta'])\n                errors.append(error)\n        else:\n            for j in range(len(layer)):\n                neuron = layer[j]\n                errors.append(neuron['output'] - expected[j])\n        for j in range(len(layer)):\n            neuron = layer[j]\n    \n \n#test backpropagation of error\nnetwork = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}]]\nexpected = [0, 1]\nbackward_propagate_error(network, expected)\nfor layer in network:\n    print(layer)","metadata":{"id":"ORlyRdz40BG-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ques** 5(20 marks) Write a simple neural network to classify given dataset. Explain the neural network created in 150 words\n\ndataset = [[2.7810836,2.550537003,0],\n\t[1.465489372,2.362125076,0],\n\t[3.396561688,4.400293529,0],\n\t[1.38807019,1.850220317,0],\n\t[3.06407232,3.005305973,0],\n\t[7.627531214,2.759262235,1],\n\t[5.332441248,2.088626775,1],\n\t[6.922596716,1.77106367,1],\n\t[8.675418651,-0.242068655,1],\n\t[7.673756466,3.508563011,1]]","metadata":{"id":"0v6V8z5a1ysk"}},{"cell_type":"code","source":"\nfrom math import exp\nfrom random import seed\nfrom random import random\n \n# Initialize a network\ndef initialize_network(n_inputs, n_hidden, n_outputs):\n    network = list()\n    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n    network.append(hidden_layer)\n    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n    network.append(output_layer)\n    return network\n \n# Calculate neuron activation for an input\ndef activate(weights, inputs):\n    activation = weights[-1]\n    for i in range(len(weights)-1):\n        activation += weights[i] * inputs[i]\n    return activation\n \n# Transfer neuron activation\ndef transfer(activation):\n    return 1.0 / (1.0 + exp(-activation))\n \n# Forward propagate input to a network output\ndef forward_propagate(network, row):\n    inputs = row\n    for layer in network:\n        new_inputs = []\n        for neuron in layer:\n            activation = activate(neuron['weights'], inputs)\n            neuron['output'] = transfer(activation)\n            new_inputs.append(neuron['output'])\n        inputs = new_inputs\n    return inputs\n \n# Calculate the derivative of an neuron output\ndef transfer_derivative(output):\n    return output * (1.0 - output)\n \n# Backpropagate error and store in neurons\ndef backward_propagate_error(network, expected):\n    for i in reversed(range(len(network))):\n        layer = network[i]\n        errors = list()\n        if i != len(network)-1:\n            for j in range(len(layer)):\n                error = 0.0\n                for neuron in network[i + 1]:\n                    error += (neuron['weights'][j] * neuron['delta'])\n                errors.append(error)\n        else:\n            for j in range(len(layer)):\n                neuron = layer[j]\n                errors.append(neuron['output'] - expected[j])\n        for j in range(len(layer)):\n            neuron = layer[j]\n            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n \n# Update network weights with error\ndef update_weights(network, row, l_rate):\n    for i in range(len(network)):\n        inputs = row[:-1]\n        if i != 0:\n            inputs = [neuron['output'] for neuron in network[i - 1]]\n        for neuron in network[i]:\n            for j in range(len(inputs)):\n                neuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n            neuron['weights'][-1] -= l_rate * neuron['delta']\n \n# Train a network for a fixed number of epochs\ndef train_network(network, train, l_rate, n_epoch, n_outputs):\n    for epoch in range(n_epoch):\n        sum_error = 0\n        for row in train:\n            outputs = forward_propagate(network, row)\n            expected = [0 for i in range(n_outputs)]\n            expected[row[-1]] = 1\n            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n            backward_propagate_error(network, expected)\n            update_weights(network, row, l_rate)\n        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n \n# Test training backprop algorithm\nseed(1)\ndataset = [[2.7810836,2.550537003,0],\n    [1.465489372,2.362125076,0],\n    [3.396561688,4.400293529,0],\n    [1.38807019,1.850220317,0],\n    [3.06407232,3.005305973,0],\n    [7.627531214,2.759262235,1],\n    [5.332441248,2.088626775,1],\n    [6.922596716,1.77106367,1],\n    [8.675418651,-0.242068655,1],\n    [7.673756466,3.508563011,1]]\nn_inputs = len(dataset[0]) - 1\nn_outputs = len(set([row[-1] for row in dataset]))\nnetwork = initialize_network(n_inputs, 2, n_outputs)\ntrain_network(network, dataset, 0.5, 20, n_outputs)\nfor layer in network:\n    print(layer)","metadata":{"id":"oxRPCqVE2YWe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thank you for completing all the questions","metadata":{}}]}